{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안철수크롤링 시작\n",
      "홍준표크롤링 시작\n",
      "유승민크롤링 시작\n",
      "박근혜크롤링 시작\n",
      "심상정크롤링 시작\n",
      "문재인크롤링 시작\n",
      "완료1\n",
      "완료2\n"
     ]
    }
   ],
   "source": [
    "# 팀작업 폴더를 생성하고 실행하고, 팀작업의 이 코드는 상위폴더에서 실행하세요\n",
    "# konlpy 설치가 필요합니다 설치하는 링크는 'konlpy 공식 홈페이지(http://konlpy.org/ko/latest/)'입니다\n",
    "# jdk 64bit 설치도 필요합니다\n",
    "import sys\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.re-quest\n",
    "from urllib.parse import quote\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    " \n",
    "TARGET_URL_BEFORE_PAGE_NUM = \"http://news.donga.com/search?p=\"\n",
    "TARGET_URL_BEFORE_KEWORD = '&query='\n",
    "TARGET_URL_REST = '&check_news=1&more=1&sorting=3&search_date=1&v1=&v2=&range=3'\n",
    "\n",
    "# 단어 수 검사\n",
    "def get_tags(text, ntags=50):\n",
    "    spliter = Twitter()\n",
    "    nouns = spliter.nouns(text)\n",
    "    count = Counter(nouns)\n",
    "    return_list = []\n",
    "    for n, c in count.most_common(ntags):\n",
    "        temp = {'tag': n, 'count': c}\n",
    "        return_list.append(temp)\n",
    "    return return_list\n",
    "\n",
    "\n",
    "# 기사 검색 페이지에서 기사 제목에 링크된 기사 본문 주소 받아오기\n",
    "def get_link_from_news_title(page_num, URL, output_file):\n",
    "    for i in range(page_num):\n",
    "        current_page_num = 1 + i*15\n",
    "        position = URL.index('=')\n",
    "        URL_with_page_num = URL[: position+1] + str(current_page_num) \\\n",
    "                            + URL[position+1 :]\n",
    "        #print(URL_with_page_num)\n",
    "        source_code_from_URL = urllib.request.urlopen(URL_with_page_num)\n",
    "        #'check_news=1'은 뉴스로 한정지은 것을, 'more=1' =더 보기 'sorting=3'과 'range=3'=정확도순 정렬, \n",
    "        #전체기간검색, 'query=' 검색어 \n",
    "        soup = BeautifulSoup(source_code_from_URL, 'lxml',\n",
    "                             from_encoding='utf-8')\n",
    "        #print('1 : '+str(source_code_from_URL) + ' 2 : '+str(soup))\n",
    "        #print('1 : '+str(source_code_from_URL))\n",
    "        #print('2 : '+str(soup))\n",
    "        #count=0\n",
    "        for title in soup.find_all('p', 'tit'):\n",
    "            #print(title)\n",
    "            #count+=1\n",
    "            #print(count)\n",
    "            title_link = title.select('a')\n",
    "            article_URL = title_link[0]['href']\n",
    "            get_text(article_URL, output_file)\n",
    " \n",
    " \n",
    "# 기사 본문 내용 긁어오기 (위 함수 내부에서 기사 본문 주소 받아 사용되는 함수)\n",
    "def get_text(URL, output_file):\n",
    "    source_code_from_url = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8')\n",
    "    content_of_article = soup.select('div.article_txt')\n",
    "    for item in content_of_article:\n",
    "        string_item = str(item.find_all(text=True))\n",
    "        string_item = clean_text(string_item)\n",
    "        output_file.write(string_item)\n",
    "# 필터링\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', cleaned_text)\n",
    "    return cleaned_text\n",
    " \n",
    "# 메인함수\n",
    "def main(argv):\n",
    "    keyword = ['안철수','홍준표','유승민','박근혜','심상정','문재인']\n",
    "    #print(len(keyword))\n",
    "    page_num = 2\n",
    "    keyword_count=0\n",
    "    txt='.txt'\n",
    "    output_file_name = keyword\n",
    "    for i in range(len(keyword)):\n",
    "        output_file = open(str(i)+txt, 'w')\n",
    "        print(keyword[i]+ '크롤링 시작')\n",
    "        target_URL = TARGET_URL_BEFORE_PAGE_NUM + TARGET_URL_BEFORE_KEWORD \\\n",
    "        + quote(keyword[i]) + TARGET_URL_REST\n",
    "        get_link_from_news_title(page_num, target_URL, output_file)\n",
    "        \n",
    "    \n",
    "    output_file.close()\n",
    "    \n",
    "    \n",
    "    noun_count = 10000\n",
    "    #output_file_name = +txt:\n",
    "    doc= '팀작업/'\n",
    "    print('완료1')\n",
    "    for i in range(len(keyword)):\n",
    "        open_text_file = open(str(i)+txt, 'r')\n",
    "        t-ext = open_text_file.read()\n",
    "        tags = get_tags(text, noun_count)\n",
    "        open_text_file.close()\n",
    "        open_output_file = open(doc+'num'+str(i)+txt, 'w')\n",
    "        for tag in tags:\n",
    "            noun = tag['tag']\n",
    "            count = tag['count']\n",
    "            open_output_file.write('{} {}\\n'.format(noun, count))\n",
    "    open_output_file.close()\n",
    "    print('완료2')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
